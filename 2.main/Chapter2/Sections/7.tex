\section{متریک های ارزیابی}

در یادگیری ماشینی، مفاهیم متفاوتی برای بررسی نتایج عملکرد مدل ها بر اساس مفاهیم آماری استفاده می شود که هر یک معنی به خصوصی دارند. در این قسمت تعدادی از مفاهیم استفاده شده در این پایان نامه را شرح می دهیم.

فرض کنید داده های مساله دو دسته ی
\lr{A}
و
\lr{B}
هستند، و ماشین بر روی این داده ها آموزش داده می شود.
سپس دسته ای از داده ها به ماشین آموزش یافته، داده می شود تا آن ها را بر چسب گذاری کند.
در این صورت برای پیش بینی برچسب
\lr{A}
چهار حالت مختلف رخ می دهد.
در صورتی که داده از کلاس
\lr{A}
باشد و ماشین هم به درستی برچسب کلاس
\lr{A}
را پیش بینی کند، این یک پیش بینی مثبت درست
\lr{(TP)}
\LTRfootnote{True-Positive}
است.
اگر داده از کلاس
\lr{B}
باشد و ماشین هم مجددا به درستی کلاس
\lr{B}
را پیش بینی کند، حالت منفی درست
\lr{(TN)}
\LTRfootnote{True-Negative}
رخ می دهد.
اگر برچسب داده
\lr{A}
باشد و ماشین اشتباها آن را به عنوان کلاس
\lr{B}
برچسب گذاری کند، حالت منفی نادرست
\lr{(FN)}
\LTRfootnote{False-Negative}
رخ داده است.
و در آخر اگه برچسب داده از نوع
\lr{B}
باشد اما ماشین اشتباها به عنوان کلاس
\lr{A}
تشخیصش بدهد، در این صورت مثبت نادرست
\lr{(FN)}
\LTRfootnote{False-Negative}
که به آژیر کاذب نیز معروف است رخ داده است.

حال با استفاده از این چهار مفهموم مختلف، انواع متریک های تعریف شده برای ارزیابی نتایج مدل را تعریف می کنیم. معروف ترین متریک برای مدل های یادگیری ماشینی صحت
\LTRfootnote{Accuracy}
است. صحت ماشین در واقع نسبت تعداد حالت هایی هستند که ماشین برچسب آن ها را به درستی تشخیص داده، نسبت به تعداد کل داده ها است. به عبارتی صحت می شود:

\begin{equation}\label{eqn:acc}
Accuracy = \frac{TP+TN}{P+N} = \frac{TP+TN}{TP+TN+FP+FN}
\end{equation}

این متریک معمولا برای زمانی که داده های هر دو کلاس فراوانی تقریبا مشابهی دارند مناسب است. اما فرض کنید میخواهیم دستگاهی داشته باشیم که قاتلی را از میان افراد یک شهر تشخیص دهد. تعداد قاتلین موجود در شهر، خیلی خیلی کمتر از تعداد کل افراد شهر است. بنابرین اگر ماشین همه را بی گناه اعلام کند، با توجه به این متریک، درصد خیلی بالایی از پیش بینی هایش درست است‌ (فرض کنید
$99.9\%$
).
اما آیا این پیش بینی ارزشی دارد؟ قطعا خیر. پس برای ارزیابی چنین مدلی به متریک دیگری احتیاج داریم.

متریک معروف دیگر، دقت
\LTRfootnote{Precision}
است. دقت در مثال قبل به این معنا است که اگر ماشین، شخصی را به عنوان قاتل اعلام کند، به چه احتمالی این شخص واقعا قاتل است. به عبارتی اگر ماشین تعداد
$n$
نفر را قاتل اعلام کند، چند نفر از این
$n$
نفر واقعا قاتل هستند. این متریک همانگونه که دیده می شود، در این مساله خیلی مهم است. به این خاطر که اگر دقت پیش بینی پایین باشد، فرد بی گناهی به عنوان قاتل شناسایی می شود. تعریف ریاضی دقت بنا به مفاهیم گفته شده به صورت زیر است:

\begin{equation}\label{eqn:precision}
Precision = \frac{TP}{TP+FP}
\end{equation}

در آخر، متریک بازیابی
\LTRfootnote{Recall}
به معنی تعداد داده تشخیص داده شده به عنوان کلاس
\lr{A}
نسبت به کل تعداد کلاس
\lr{A}
است. در مثال قبل، بازیابی در واقع تعداد افراد قاتل تشخیص داده شده، نسبت به کل تعداد قاتلین شهر است. به صورت ریاضی، بازیابی عبارت است از:

\begin{equation}\label{eqn:recall}
Recall = \frac{TP}{P}
\end{equation}


متریک های دقت و بازیابی با دیگر مصالحه دارند. به این معنی که می توان با  افزایش آستانه ی مدل برای شناسایی
\lr{A}
، میزان دقت را بالا برد، اما معمولا  در این حالت میزان بازیابی کم می شود. همچنین اگه آستانه مدل برای تشخیص حالت
\lr{A}
را کم کنیم، مدل بازیابی بیشتری پیدا میکند، اما معمولا دقت کم می شود.
پس با تنظیم آستانه مدل، می توان به مقدار دقت دلخواه یا بازیابی دلخواه دست یافت. این مساله در شکل
\ref{fig:precision_recall_threshold}
قابل مشاهده است.

\input{\figurePATH{2}{precision_recall_threshold}}

%%%%%%%
\subsection{ماتریس درهم ریختگی}

ماتریس درهم ریختگی
\LTRfootnote{Confusion Matrix}
یا ماتریس خطا، جدولی برای پیش بینی مدل در حوزه ی یادگیری ماشینی است. در این ماتریس هر سطر بیانگر یک کلاس تشخیص داده شده توسط ماشین، و هر ستون بیانگر یک کلاس واقعی داده ها است. در حالت ایده آل که همه ی حالت ها توسط ماشین درست تشخیص داده شده اند، این ماتریس قطری می شود. در جدول
\ref{tab:conf_matrix_def}
این ماتریس بر حسب چهار حالت گفته شده در بخش قبل توصیف شده است.

\input{\tablePATH{2}{conf_matrix_def}}
