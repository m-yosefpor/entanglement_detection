\section{شبکه های عصبی}

\input{\figurePATH{2}{nn_arch}}

این روش که الهام گرفته شده از شکبه عصبی موجودات زنده است، شبکه ای از گره ها و یال ها است که به صورت لایه لایه مطابق شکل
\ref{fig:fnn_arch}
است
\cite{chen2019}
و به این صورت عمل می کند که ویژگی های موجود به صورت پارامتر های اولیه به لایه ی اول شبکه داده می شود.
گره ها هر کدام مانند حافظه ای هستند که می توانند یک عدد اعشاری را در خود ذخیره کنند. هر یک از یال ها، در واقع ضریب ضرب شونده در گره لایه ی قبل است. در نتیجه مقدار اولیه هر گره در لایه ی بعد،‌ترکیب خطی از مقادیر گره های لایه های قبل است. پس از به دست آمدن این ترکیب خطی، یک تابع غیر خطی بر این مقدار اعمال می شود که به این تابع غیر خطی،‌ تابع فعال سازی
\LTRfootnote{Activation Function}
گفته می شود. توابع متفاوتی برای تابع فعال سازی استفاده می شود که از جمله آن ها می توان به توابع زیر اشاره کرد:


\begin{align}
	\text{ReLU}	  	& = \max{\{0,x\}} \\
	\text{Sigmoid}	  	& = \frac{1}{1+e^{-x}} \\
	\text{Gaussian}    & = e^{-x^2	}
\end{align}
