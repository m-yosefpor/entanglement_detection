\section{روش های کلاسیک}

در این بخش با استفاده از روش های کلاسیکی یادگیری ماشینی، به تفکیک حالت های جداپذیر و در هم تنیده پرداخته ایم. روش انجام شده در اینجا به دو دسته کلی تقسیم می شود. دسته اول مساله دسته بندی
\LTRfootnote{Classificaion}
با استفاده از برچسب ۰ و ۱ (درهم تنیده یا جداپذیر بودن است)
در این مساله میزان درهم تنیدگی به ماشین داده نمی شود. در دسته دوم، یک مساله ی رگرسیون با استفاده از دترمینان ماتریس ترانهاده جزئی انجام می دهیم، به طوری که ۱۵ ویژگی به همراه دترمینان ماتریس ترانهاده جزئی به ماشین داده می شود، و ماشین با استفاده از این اعداد سعی در یادگیری میزان درهم تنیده بودن میکند. در آخر، زمانی که ماشین آموزش داده شده حالت جدیدی را پیش بینی کرد، با توجه به مثبت یا منفی بودن این عدد، در هم تنیده بودن یا جدا پذیر بودن حالت را استنباط می کنیم. پس در واقع از یک رگرسور،‌ یک تفکیک گر
\LTRfootnote{Classifier}
می سازیم.

هر یک از این دسته ها با روش های متعددی انجام شده است که نتایج هر یک و زمان یادگیری و پیش بینی آن ها در جدول
\ref{tab:est_info}
آورده شده است.

\input{\tablePATH{3}{est_info}}

%%%%%%%%%%%%%%%%
\subsection{اهمیت ویژگی ها}
در این روش های کلاسیکی، از اندازه ضرایب به دست آمده می توان اهمیت هر یک از ویژگی ها را به دست آورد. همچینی روش های درختی اهمیت ویژگی ها را ارایه می دهند.
هرچه اهمیت یک ویژگی بیشتر باشد، حاوی اطلاعات بیشتری برای تفکیک مساله به دست می دهد. بنابرین اگه ویژگی های با اهمیت بیشتر حذف شوند، صحت تفکیک افت بیشتری می کند.

در بررسی صورت گرفته مشاهده کردیم که همه ی ویژگی ها به یک انداره مهم نیستند و برخی از آن ها حاوی اطلاعات بیشتری برای تفکیک حالت های جداپذیر و در هم تنیده هستند.
ویژگی هایی که شامل ضرب تانسوری دو ماتریس پاولی یکسان بودند، همه در رده ی اول اهمیت هستند. این ها عبارتند از
$\sigma_x \otimes \sigma_x , \sigma_y \otimes \sigma_y , \sigma_z \otimes \sigma_z$.
پس از این ها، ویژگی هایی که شامل ضرب تانسوری دو ماتریس پاولی در ابعاد مختلف هستند، دارای اهمیت رده ی دوم می باشند، به عنوان مثال
$\sigma_x \otimes \sigma_y$ , $\sigma_x \otimes \sigma_z$.
در آخر نیز ویژگی هایی که شامل ضرب یک ماتریس پاولی و یک عملگر همانی اند در رده سوم و آخر اهمیت هستند.

این اهمیت ویژگی ها برای یک نمونه از روش تفکیک تصادفی درختی در شکل
\ref{fig:feature_importance}
آورده شده است.

\input{\figurePATH{3}{feature_importance}}
